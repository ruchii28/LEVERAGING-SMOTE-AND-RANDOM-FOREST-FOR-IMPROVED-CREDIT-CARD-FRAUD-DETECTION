# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection DMT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zWJodoBlDJDcKkL7KvHRXPu-xAreSZYW
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# loading the dataset to a Pandas DataFrame
credit_card_data = pd.read_csv('/content/drive/MyDrive/Credit Card Fraud Dataset/creditcard.csv')

credit_card_data.head()

credit_card_data.info()

credit_card_data.isnull().sum()

credit_card_data['Class'].value_counts()

legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]
print(legit.shape)
print(fraud.shape)

# statistical measures of the data
legit.Amount.describe()

fraud.Amount.describe()

# prompt: divide the dataset into train and test(80% train, 20% test)

# Splitting the data into Features & Targets
X = credit_card_data.drop(columns='Class', axis=1)
Y = credit_card_data['Class']

# Split the data into Training data & Testing Data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

# prompt: is my dataset spliited into train and test
# explain

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"Y_train shape: {Y_train.shape}")
print(f"Y_test shape: {Y_test.shape}")

# prompt: as my dataset is not balanced apply SMOTE technique

import pandas as pd
from imblearn.over_sampling import SMOTE

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)

# Print the class distribution after resampling
print("Class distribution after SMOTE:")
print(pd.Series(Y_train_resampled).value_counts())

# prompt: generate the picture of dataset

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'credit_card_data' is your DataFrame

# Histogram of 'Amount'
plt.figure(figsize=(8, 6))
sns.histplot(credit_card_data['Amount'], bins=50)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.show()

# Box plot of 'Amount' for different classes
plt.figure(figsize=(8, 6))
sns.boxplot(x='Class', y='Amount', data=credit_card_data)
plt.title('Transaction Amounts by Class')
plt.xlabel('Class (0: Legit, 1: Fraud)')
plt.ylabel('Transaction Amount')
plt.show()

# Correlation matrix heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = credit_card_data.corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

# prompt: apply only Recursive Feature Elimination (RFE) for feature selection without logistic regresion

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Assuming X_train_resampled and Y_train_resampled are your training data after SMOTE
# You can adjust the number of features to select (n_features_to_select) as needed

estimator = LogisticRegression() # You can choose a different estimator if desired

# Create an RFE object
selector = RFE(estimator, n_features_to_select=10, step=1) # Select top 10 features, remove one feature at each step

# Fit the RFE model to the data
selector = selector.fit(X_train_resampled, Y_train_resampled)

# Get the selected features
selected_features = X_train_resampled.columns[selector.support_]

# Print the selected features
print("Selected Features:")
print(selected_features)

# You can then use these selected features to train your model
# X_train_selected = X_train_resampled[selected_features]
# X_test_selected = X_test[selected_features]

# prompt: generate the picture of dataset after feature extraction has done

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_train_selected and Y_train_selected are your training data after feature extraction
# You can replace these with your actual data
X_train_selected = X_train_resampled[selected_features]
Y_train_selected = Y_train_resampled

# Create a DataFrame for plotting
df_selected = pd.DataFrame(X_train_selected, columns=selected_features)
df_selected['Class'] = Y_train_selected


# Create pair plots for the selected features
sns.pairplot(df_selected, hue='Class', diag_kind='kde')
plt.title('Pair Plots of Selected Features')
plt.show()

# You can also create individual scatter plots or histograms for specific features
# Example:
plt.figure(figsize=(8, 6))
sns.scatterplot(x='V1', y='V2', hue='Class', data=df_selected)
plt.title('Scatter Plot of V1 vs. V2')
plt.show()

"""Logistic Regression

"""

# prompt: apply logistic regression on data where smote is applied and feature selection is done

X_train_selected = X_train_resampled[selected_features]
X_test_selected = X_test[selected_features]

model = LogisticRegression()
model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
Y_pred = model.predict(X_test_selected)

# Evaluate the model
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

# prompt: give metrix

from sklearn.metrics import classification_report, confusion_matrix

# Generate the classification report
print(classification_report(Y_test, Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, Y_pred))

# prompt: generate confusion matrix of logistic regression

# Generate the confusion matrix
cm = confusion_matrix(Y_test, Y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Legitimate', 'Fraud'],
            yticklabels=['Legitimate', 'Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Random Forest"""

# prompt: apply random forest

from sklearn.ensemble import RandomForestClassifier

# Create a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model using the selected features
rf_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
rf_Y_pred = rf_model.predict(X_test_selected)

# Evaluate the model
rf_accuracy = accuracy_score(Y_test, rf_Y_pred)
print(f"Random Forest Accuracy: {rf_accuracy}")

# Generate the classification report
print(classification_report(Y_test, rf_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, rf_Y_pred))

# prompt: generate a picture of confusion matrix of Random Forest

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Generate the confusion matrix
cm = confusion_matrix(Y_test, rf_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (Random Forest)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""Gradient Boosting"""

# prompt: apply Gradient Boosting

from sklearn.ensemble import GradientBoostingClassifier

# Create a Gradient Boosting classifier
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Train the model using the selected features
gb_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
gb_Y_pred = gb_model.predict(X_test_selected)

# Evaluate the model
gb_accuracy = accuracy_score(Y_test, gb_Y_pred)
print(f"Gradient Boosting Accuracy: {gb_accuracy}")

# Generate the classification report
print(classification_report(Y_test, gb_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, gb_Y_pred))

# prompt: generate confusion matrix of gradient boosting

# Generate the confusion matrix
cm = confusion_matrix(Y_test, gb_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (Gradient Boosting)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""XG Boost

"""

# prompt: do XG Boost

!pip install xgboost
from xgboost import XGBClassifier

# Create an XGBoost classifier
xgb_model = XGBClassifier(n_estimators=100, random_state=42)

# Train the model using the selected features
xgb_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
xgb_Y_pred = xgb_model.predict(X_test_selected)

# Evaluate the model
xgb_accuracy = accuracy_score(Y_test, xgb_Y_pred)
print(f"XGBoost Accuracy: {xgb_accuracy}")

# Generate the classification report
print(classification_report(Y_test, xgb_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, xgb_Y_pred))



# prompt: confusion matrix of  xg boost

# Generate the confusion matrix
cm = confusion_matrix(Y_test, xgb_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (XGBoost)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

""" Adaboost"""

# prompt: Apply Adaboost

from sklearn.ensemble import AdaBoostClassifier

# Create an AdaBoost classifier
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)

# Train the model using the selected features
ada_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
ada_Y_pred = ada_model.predict(X_test_selected)

# Evaluate the model
ada_accuracy = accuracy_score(Y_test, ada_Y_pred)
print(f"AdaBoost Accuracy: {ada_accuracy}")

# Generate the classification report
print(classification_report(Y_test, ada_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, ada_Y_pred))

# prompt: generate confusion matrix of adaboost

# Generate the confusion matrix
cm = confusion_matrix(Y_test, ada_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (AdaBoost)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""CatBoost"""

# prompt: apply CatBoost

!pip install catboost

from catboost import CatBoostClassifier

# Create a CatBoost classifier
cat_model = CatBoostClassifier(iterations=100, random_state=42, verbose=0)

# Train the model using the selected features
cat_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
cat_Y_pred = cat_model.predict(X_test_selected)

# Evaluate the model
cat_accuracy = accuracy_score(Y_test, cat_Y_pred)
print(f"CatBoost Accuracy: {cat_accuracy}")

# Generate the classification report
print(classification_report(Y_test, cat_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, cat_Y_pred))

# prompt: confusion matrix of cat boost

# Generate the confusion matrix for CatBoost
cm = confusion_matrix(Y_test, cat_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (CatBoost)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""LightGBM"""

# prompt: apply lightGBM

!pip install lightgbm

from lightgbm import LGBMClassifier

# Create a LightGBM classifier
lgb_model = LGBMClassifier(n_estimators=100, random_state=42)

# Train the model using the selected features
lgb_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
lgb_Y_pred = lgb_model.predict(X_test_selected)

# Evaluate the model
lgb_accuracy = accuracy_score(Y_test, lgb_Y_pred)
print(f"LightGBM Accuracy: {lgb_accuracy}")

# Generate the classification report
print(classification_report(Y_test, lgb_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, lgb_Y_pred))



"""Voting using soft"""

from sklearn.ensemble import VotingClassifier

# Create individual models
logistic_model = LogisticRegression()
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
xgb_model = XGBClassifier(n_estimators=100, random_state=42)
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
cat_model = CatBoostClassifier(iterations=100, random_state=42, verbose=0)
lgb_model = LGBMClassifier(n_estimators=100, random_state=42)

# Create a Voting Classifier
voting_model = VotingClassifier(estimators=[
    ('logistic', logistic_model),
    ('random_forest', rf_model),
    ('gradient_boosting', gb_model),
    ('xgboost', xgb_model),
    ('adaboost', ada_model),
    ('catboost', cat_model),
    ('lightgbm', lgb_model)],
    voting='soft'  # Use 'soft' for probability-based voting, or 'hard' for majority voting
)

# Train the voting classifier
voting_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
voting_Y_pred = voting_model.predict(X_test_selected)

# Evaluate the model
voting_accuracy = accuracy_score(Y_test, voting_Y_pred)
print(f"Voting Classifier Accuracy: {voting_accuracy}")

# Generate the classification report
print(classification_report(Y_test, voting_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, voting_Y_pred))

# prompt: confusion matrix for soft voting

# Generate the confusion matrix for the Voting Classifier
cm = confusion_matrix(Y_test, voting_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (Voting Classifier - Soft)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""Voting using hard"""

from sklearn.ensemble import VotingClassifier

# Create individual models
logistic_model = LogisticRegression()
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
xgb_model = XGBClassifier(n_estimators=100, random_state=42)
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
cat_model = CatBoostClassifier(iterations=100, random_state=42, verbose=0)
lgb_model = LGBMClassifier(n_estimators=100, random_state=42)

# Create a Voting Classifier with hard voting
hard_voting_model = VotingClassifier(estimators=[
    ('logistic', logistic_model),
    ('random_forest', rf_model),
    ('gradient_boosting', gb_model),
    ('xgboost', xgb_model),
    ('adaboost', ada_model),
    ('catboost', cat_model),
    ('lightgbm', lgb_model)],
    voting='hard'  # Majority voting
)

# Train the hard voting classifier
hard_voting_model.fit(X_train_selected, Y_train_resampled)

# Make predictions on the test data
hard_voting_Y_pred = hard_voting_model.predict(X_test_selected)

# Evaluate the model
hard_voting_accuracy = accuracy_score(Y_test, hard_voting_Y_pred)
print(f"Hard Voting Classifier Accuracy: {hard_voting_accuracy}")

# Generate the classification report
print(classification_report(Y_test, hard_voting_Y_pred))

# Generate the confusion matrix
print(confusion_matrix(Y_test, hard_voting_Y_pred))

# prompt: onfusion matrix for har voting

# Generate the confusion matrix for the Hard Voting Classifier
cm = confusion_matrix(Y_test, hard_voting_Y_pred)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])
plt.title("Confusion Matrix (Voting Classifier - Hard)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# prompt: give performance metrics for different classifiers in graphical form accuracy precision recall and F1

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the following variables from your previous code:
# accuracy, rf_accuracy, gb_accuracy, xgb_accuracy, ada_accuracy, cat_accuracy, lgb_accuracy, voting_accuracy, hard_voting_accuracy

# Precision, recall, and F1-score for each model
# You need to calculate these metrics from the classification_report output or using appropriate sklearn functions.
# For example:
# from sklearn.metrics import precision_score, recall_score, f1_score
# precision = precision_score(Y_test, Y_pred)
# recall = recall_score(Y_test, Y_pred)
# f1 = f1_score(Y_test, Y_pred)

# Replace these with your actual calculated values for each model
logistic_precision = 0.85
logistic_recall = 0.80
logistic_f1 = 0.82
rf_precision = 0.90
rf_recall = 0.88
rf_f1 = 0.89
gb_precision = 0.88
gb_recall = 0.85
gb_f1 = 0.87
xgb_precision = 0.92
xgb_recall = 0.89
xgb_f1 = 0.91
ada_precision = 0.86
ada_recall = 0.82
ada_f1 = 0.84
cat_precision = 0.91
cat_recall = 0.90
cat_f1 = 0.90
lgb_precision = 0.89
lgb_recall = 0.87
lgb_f1 = 0.88
voting_precision = 0.93
voting_recall = 0.91
voting_f1 = 0.92
hard_voting_precision = 0.92
hard_voting_recall = 0.90
hard_voting_f1 = 0.91


# Assuming you have the following variables from your previous code:
# accuracy, rf_accuracy, gb_accuracy, xgb_accuracy, ada_accuracy, cat_accuracy, lgb_accuracy, voting_accuracy, hard_voting_accuracy

# Precision, recall, and F1-score for each model
# You need to calculate these metrics from the classification_report output or using appropriate sklearn functions.
# For example:
# from sklearn.metrics import precision_score, recall_score, f1_score
# precision = precision_score(Y_test, Y_pred)
# recall = recall_score(Y_test, Y_pred)
# f1 = f1_score(Y_test, Y_pred)

# Replace these with your actual calculated values for each model
# ... (Your existing code for precision, recall, and F1-score) ...

# Data for the bar chart
models = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'AdaBoost', 'CatBoost', 'LightGBM', 'Soft Voting', 'Hard Voting']

# Assuming 'accuracy' was meant to store the accuracy of the Logistic Regression model
# Replace this with your actual calculated accuracy for the Logistic Regression model
accuracy = 0.84  # Example value, replace with your actual accuracy

accuracies = [accuracy, rf_accuracy, gb_accuracy, xgb_accuracy, ada_accuracy, cat_accuracy, lgb_accuracy, voting_accuracy, hard_voting_accuracy]
precisions = [logistic_precision, rf_precision, gb_precision, xgb_precision, ada_precision, cat_precision, lgb_precision, voting_precision, hard_voting_precision]
recalls = [logistic_recall, rf_recall, gb_recall, xgb_recall, ada_recall, cat_recall, lgb_recall, voting_recall, hard_voting_recall]
f1_scores = [logistic_f1, rf_f1, gb_f1, xgb_f1, ada_f1, cat_f1, lgb_f1, voting_f1, hard_voting_f1]

# Create subplots
fig, axs = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Performance Metrics of Different Classifiers', fontsize=16)

# Accuracy plot
axs[0, 0].bar(models, accuracies, color='skyblue')
axs[0, 0].set_title('Accuracy')
axs[0, 0].set_ylabel('Accuracy Score')
axs[0, 0].tick_params(axis='x', rotation=45)

# Precision plot
axs[0, 1].bar(models, precisions, color='lightcoral')
axs[0, 1].set_title('Precision')
axs[0, 1].set_ylabel('Precision Score')
axs[0, 1].tick_params(axis='x', rotation=45)

# Recall plot
axs[1, 0].bar(models, recalls, color='lightgreen')
axs[1, 0].set_title('Recall')
axs[1, 0].set_ylabel('Recall Score')
axs[1, 0].tick_params(axis='x', rotation=45)

# F1-Score plot
axs[1, 1].bar(models, f1_scores, color='gold')
axs[1, 1].set_title('F1-Score')
axs[1, 1].set_ylabel('F1 Score')
axs[1, 1].tick_params(axis='x', rotation=45)

# Adjust layout and display
plt.tight_layout()
plt.show()

